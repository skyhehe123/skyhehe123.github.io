<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Chenhang HE</title>

    <meta name="author" content="Chenhang HE">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

<body>
  
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Chenhang HE
                </p>
                <p>I'm a research assistant professor at <a href="https://www.polyu.edu.hk/comp/">The Department of Computing, Hong Kong Polytechnic University. </a> I have received my PhD at COMP@PolyU, working with Prof. Lei Zhang. I have received my MPhil at EIE@PolyU, working with Prof. Kin-Man Lam.
                </p>
               
                <p style="text-align:center">
                  <a href="mailto:chenhang.he@polyu.edu.hk">Email</a> &nbsp;/&nbsp;
                  <a href="doc/ChenhangCV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=dU6hpFUAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/skyhehe123">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/chenhang-he-a653281a6">Linkedin</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="imgs/profile_circ.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="imgs/profile_circ.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
     </tbody>
  </table>
          
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <h2>Join Us!</h2> 
          <p>
            We have multiple PhD openings available for Spring and Fall 2026. If you are interested, please send your CV.
          </p>
          
          <h2>Research</h2>
          
          <p>
              My research mainly focus on Point Cloud 3D Object Detection, Instance Segmentation and 2D Object Detection.
          </p>
          <p>
              <a href="https://arxiv.org/pdf/2502.01084">
                <papertitle>Continuous Autoregressive Modeling with Stochastic Monotonic Alignment for Speech Synthesis</papertitle>
              </a>
              <br>
              <a>Weiwei Lin</a>
              <a>Chenhang He</a>,
              <br>
              <em>ICLR</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2502.01084">paper</a> 
              <br>
              
          </p>
          <p>
              <a href="https://arxiv.org/abs/2406.10700">
                <papertitle>Voxel Mamba: Group-Free State Space Models for Point Cloud based 3D Object Detection</papertitle>
              </a>
              <br>
              <a>Guowen Zhang</a>
              <a>Lue Fan</a>
              <a>Chenhang He</a>,
              <a>Zhen Lei</a>,
              <a>Zhaoxiang Zhang</a>,
              <a>Lei Zhang</a>
              <br>
              <em>NeurIPS</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2406.10700">paper</a> /
              <a href="https://github.com/gwenzhang/Voxel-Mamba">code</a>
              <br>
              
          </p>

          <p>
              <a href="https://arxiv.org/html/2401.00912v2">
                <papertitle>ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention</papertitle>
              </a>
              <br>
              <a>Chenhang He</a>,
              <a>Ruihuang Li</a>,
              <a>Guowen Zhang</a>,
              <a>Lei Zhang</a>
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/html/2401.00912v2">paper</a> /
              <a href="https://github.com/skyhehe123/ScatterFormer">code</a>
              <br>
              
          </p>

          <p>
              <a href="https://arxiv.org/pdf/2312.00853">
                <papertitle>Motion-Guided Latent Diffusion for Temporally Consistent Real-world Video Super-resolution</papertitle>
              </a>
              <br>
              <a>Xi Yang</a>,
              <a>Chenhang He</a>,
              <a>Jianqi Ma</a>,
              <a>Lei Zhang</a>
              <br>
              <em>ECCV</em>, 2024
            
              <br>
              <a href="https://arxiv.org/pdf/2312.00853">paper</a> /
              <a href="https://github.com/IanYeung/MGLD-VSR">code</a>
              <br>
              
          </p>

          <p>
              <a href="https://arxiv.org/abs/2407.09781">
                <papertitle>Dense Multimodal Alignment for Open-Vocabulary 3D Scene Understanding</papertitle>
              </a>
              <br>
              <a>Ruihuang Li</a>,
              <a>Zhengqian Zhang</a>,
              <a>Chenhang He</a>,
              <a>Zhiyuan Ma</a>,
              <a>Vishal M. Patel</a>,
              <a>Lei Zhang</a>
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2407.09781">paper</a> /
              <a href="https://github.com/lslrh/DMA">code</a> 
              <br>
              
          </p>

          
          <p>
              <a href="https://arxiv.org/abs/2407.08966">
                <papertitle>LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models</papertitle>
              </a>
              <br>
              <a>Yabin Zhang</a>,
              <a>Wenjie Zhu</a>,
              <a>Chenhang He</a>,
              <a>Lei Zhang</a>
              <br>
              <em>ECCV</em>, 2024
         
              <br>
              <a href="https://arxiv.org/abs/2407.08966">paper</a> /
              <a href="https://github.com/YBZh/LAPT">code</a> 
      
              <br>
              
          </p>
              

          <p>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_MSF_Motion-Guided_Sequential_Fusion_for_Efficient_3D_Object_Detection_From_CVPR_2023_paper.pdf">
                <papertitle>MSF: Motion-guided Sequential Fusion for Efficient 3D Object Detection from Point Cloud Sequences</papertitle>
              </a>
              <br>
              <strong>Chenhang He</strong>,
              <a>Ruihuang Li</a>,
              <a>Yabin Zhang</a>,
              <a>Shuai Li</a>,
              <a>Lei Zhang</a>
              <br>
              <em>CVPR</em>, 2023
              <!-- <em>CVPR</em>, 2022 &nbsp <font color=#FF8080><strong>(Oral)</strong></font> -->
              <!-- <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
              <br>
              <a href="doc/MSF.pdf">paper</a> /
              <a href="https://github.com/skyhehe123/MSF">code</a> /
              <a href="doc/MSF.bib">bibtex</a>
              <br>
              
          </p>

          
          <p>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Voxel_Set_Transformer_A_Set-to-Set_Approach_to_3D_Object_Detection_CVPR_2022_paper.pdf">
                <papertitle>Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds</papertitle>
              </a>
              <br>
              <strong>Chenhang He</strong>,
              <a>Ruihuang Li</a>,
              <a>Shuai Li</a>,
              <a>Lei Zhang</a>
              <br>
              <em>CVPR</em>, 2022 &nbsp 
              <!-- <em>CVPR</em>, 2022 &nbsp <font color=#FF8080><strong>(Oral)</strong></font> -->
              <!-- <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Voxel_Set_Transformer_A_Set-to-Set_Approach_to_3D_Object_Detection_CVPR_2022_paper.pdf">paper</a> /
              <a href="https://github.com/skyhehe123/VoxSeT">code</a> /
              <a href="doc/voxset.bib">bibtex</a>
              <br>
          </p>

          <p>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Structure_Aware_Single-Stage_3D_Object_Detection_From_Point_Cloud_CVPR_2020_paper.pdf">
              <papertitle><font color="#1772d0">Structure Aware Single-stage 3D Object Detection from Point Cloud</font></papertitle>
            </a>
            <br>
            <strong>Chenhang He</strong>,
            <a>Hui Zeng</a>,
            <a>Jianqiang Huang</a>,
            <a>Xiansheng Hua</a>,
            <a>Lei Zhang</a>
            <br>
            <em>CVPR</em>, 2020 <font color=#FF8080><strong>(The first place on KITTI BEV Detection )</strong></font>
            <br>
            <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Structure_Aware_Single-Stage_3D_Object_Detection_From_Point_Cloud_CVPR_2020_paper.pdf">paper</a>&nbsp/&nbsp 
            <a href="https://github.com/skyhehe123/SA-SSD">code</a>&nbsp/&nbsp 
            <a href="https://www.youtube.com/watch?v=ZvBszVt3MYw">video</a>&nbsp/&nbsp 
            <!-- <a href="https://youtu.be/zBSH-k9GbV4">video</a> -->
            <a href="doc/SASSD.bib">bibtex</a>
            <!-- <p><font color="#FF8080"><em>(&#8656 Move your cursor on the image for a short demo video!)</em></font></p> -->
            <!-- <p><font color="#009A17"><em>(&#8656 Move your cursor on the image for a short demo video!)</em></font></p> -->
         </p>
         


         <p>
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SIM_Semantic-Aware_Instance_Mask_Generation_for_Box-Supervised_Instance_Segmentation_CVPR_2023_paper.pdf">
              <papertitle>SIM: Semantic-aware Instance Mask Generation for Box-Supervised Instance Segmentation</papertitle>
            </a>
            <br>
            <a>Ruihuang Li*</a>,
            <strong>Chenhang He*</strong>,
            <a>Yabin Zhang</a>,
            <a>Shuai Li</a>,
            <a>Liyi Chen</a>,
            <a>Lei Zhang</a>
            <br>
            <em>CVPR</em>, 2023
           <font color="DarkMagenta">(* Equal Contribution)</font>
            <br>
            <a href="https://arxiv.org/pdf/2303.08578.pdf">paper</a> /
            <a href="https://github.com/lslrh/SIM">code</a> /
            <a href="doc/SIM.bib">bibtex</a>
            <br>
        </p>



        <p>
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DynaMask_Dynamic_Mask_Selection_for_Instance_Segmentation_CVPR_2023_paper.pdf">
              <papertitle>DynaMask: Dynamic Mask Selection for Instance Segmentation</papertitle>
            </a>
            <br>
            <a>Ruihuang Li*</a>,
            <strong>Chenhang He*</strong>,
            <a>Shuai Li</a>,
            <a>Yabin Zhang</a>,
            <a>Lei Zhang</a>
            <br>
            <em>CVPR</em>, 2023
           <font color="DarkMagenta">(* Equal Contribution)</font>
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DynaMask_Dynamic_Mask_Selection_for_Instance_Segmentation_CVPR_2023_paper.pdf">paper</a> /
            <a href="https://github.com/lslrh/DynaMask">code</a> /
            <a href="doc/DynaMask.bib">bibtex</a>
            <br>
        </p>

         <p>
            <a href="https://arxiv.org/pdf/2305.08099.pdf">
              <papertitle>Self-supervised Neural Factor Analysis for Disentangling</papertitle>
            </a>
            <br>
            <a>Weiwei Lin*</a>,
            <strong>Chenhang He*</strong>,
            <a>Man-Wai Mak</a>,
            <a>Youzhi Tu</a>
            <br>
            <em>ICML</em>, 2023
           <font color="DarkMagenta">(* Equal Contribution)</font>
            <br>
            <a href="https://arxiv.org/pdf/2305.08099.pdf">paper</a> 
            <br>
        </p>

         <p>
            <a href="https://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR23_o2f.pdf">
              <papertitle>One-to-Few Label Assignment for End-to-End Dense Detection</papertitle>
            </a>
            <br>
            <a>Shuai Li</a>,
            <a>Minghan Li</a>,
            <a>Ruihuang Li</a>,
            <strong>Chenhang He</strong>,
            <a>Lei Zhang</a>
            <br>
            <em>CVPR</em>, 2023
            <br>
            <a href="https://web.comp.polyu.edu.hk/cslzhang/paper/CVPR23_o2f.pdf">paper</a> /
            <a href="https://web.comp.polyu.edu.hk/cslzhang/paper/CVPR23_o2f.pdf">code</a> /
            <a href="doc/O2F.bib">bibtex</a>
            <br>
         </p>
          
        <p>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_A_Dual_Weighting_Label_Assignment_Scheme_for_Object_Detection_CVPR_2022_paper.pdf">
              <papertitle>A Dual Weighting Label Assignment Scheme for Object Detection</papertitle>
            </a>
            <br>
            <a>Shuai Li</a>,
            <strong>Chenhang He</strong>,
            <a>Ruihuang Li</a>,
            <a>Lei Zhang</a>
            <br>
            <em>CVPR</em>, 2022
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_A_Dual_Weighting_Label_Assignment_Scheme_for_Object_Detection_CVPR_2022_paper.pdf">paper</a> /
            <a href="https://github.com/strongwolf/DW">code</a> /
            <a href="doc/DW.bib">bibtex</a>
            <br>
        </p>



        <p>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Class-Balanced_Pixel-Level_Self-Labeling_for_Domain_Adaptive_Semantic_Segmentation_CVPR_2022_paper.pdf">
              <papertitle>Class-Balanced Pixel-Level Self-Labeling for Domain Adaptive Semantic Segmentation</papertitle>
            </a>
            <br>
            <a>Ruihuang Li</a>
            <a>Shuai Li</a>,
            <strong>Chenhang He</strong>,
            <a>Yabin Zhang</a>,
            <a>Xu Jia</a>
            <a>Lei Zhang</a>
            <br>
            <em>CVPR</em>, 2022
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Class-Balanced_Pixel-Level_Self-Labeling_for_Domain_Adaptive_Semantic_Segmentation_CVPR_2022_paper.pdf">paper</a> /
            <a href="https://github.com/lslrh/CPSL">code</a> /
            <a href="doc/CPSL.bib">bibtex</a>
            <br>
        </p>


        <P>
            <a href="https://ieeexplore.ieee.org/document/8682335">
              <papertitle>Improving Object Detection with Relation Graph Inference</papertitle>
            </a>
            <br>
            <strong>Chenhang He</strong>,
            <a>Shun-Cheung Lai</a>,
            <a>Kin-man Lam</a>
            <br>
            <em>ICASSP</em>, 2019
            <br>
            <a href="https://ieeexplore.ieee.org/document/8682335">paper</a> /
            <a href="doc/RGC.bib">bibtex</a>
            <br>
        </P>
          
       <P>
            <a href="https://ieeexplore.ieee.org/document/8803782">
              <papertitle>Low-resolution face recognition based on identity-preserved face hallucination</papertitle>
            </a>
            <br>
            <a href="https://johnnysclai.com/">Shun-Cheung Lai</a>,
            <strong>Chenhang He</strong>,
            <a href="http://www.eie.polyu.edu.hk/~enkmlam/">Kin-Man Lam</a>
            <br>
            <em>ICIP</em>, 2019
             <!-- &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font> -->
            <br>
            <a href="https://ieeexplore.ieee.org/document/8803782">paper</a> / 
            <a href="https://github.com/johnnysclai/SR_LRFR">code</a> /
            <a href="doc/LRFR.bib">bibtex</a>
        </p>
        
                
            </td>
          </tr>      
          
        </tbody></table>

          


      
      </td>
    </tr>
  </table>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=n&d=n1fOXznBosr2y293yDqrENBOEnrxvk5pP0_GLQdWU9o&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
 
</body>

</html>
